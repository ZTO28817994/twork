import os
import re
import json
import jieba
from dotenv import load_dotenv

if not os.getenv('GITHUB_ACTIONS'):
    load_dotenv()

from peewee import *
from model.mysql_models import (
    DB_MYSQL, Video, Document, SoraContent, Sora, SoraMedia, FileTag, Tag, init_mysql
)

SYNC_TO_POSTGRES = os.getenv('SYNC_TO_POSTGRES', 'false').lower() == 'true'
BATCH_LIMIT = None
# ÂàùÂßãÂåñ MySQLÔºàÂøÖÈ°ªÂÖàÊâßË°åÔºâ
init_mysql()

# Â¶ÇÈúÄ PostgreSQLÔºåÂÜçÂØºÂÖ•Âπ∂ÂàùÂßãÂåñ
if SYNC_TO_POSTGRES:
    from model.pg_models import DB_PG, SoraContentPg, SoraMediaPg, init_postgres
    from playhouse.shortcuts import model_to_dict
    init_postgres()

# Âêå‰πâËØçÂ≠óÂÖ∏
SYNONYM = {
    "ÊªëÈº†": "Èº†Ê†á",
    "Ëê§Âπï": "ÊòæÁ§∫Âô®",
    "Á¨îÁîµ": "Á¨îËÆ∞Êú¨",
}

def clean_text(original_string):
    target_strings = ["- Advertisement - No Guarantee", "- ÂπøÂëä - Êó†ÊãÖ‰øù"]
    for target in target_strings:
        pos = original_string.find(target)
        if pos != -1:
            original_string = original_string[:pos]

    replace_texts = [
        "Ê±ÇÊâìËµè", "Ê±ÇËµè", "ÂèØÈÄöËøá‰ª•‰∏ãÊñπÂºèËé∑ÂèñÊàñÂàÜ‰∫´Êñá‰ª∂",
        "ÁßÅËÅäÊ®°ÂºèÔºöÂ∞ÜÂê´ÊúâFile IDÁöÑÊñáÊú¨Áõ¥Êé•ÂèëÈÄÅÁªôÊú∫Âô®‰∫∫ @datapanbot Âç≥ÂèØËøõË°åÊñá‰ª∂Ëß£Êûê",
        "‚ë†ÁßÅËÅäÊ®°ÂºèÔºöÂ∞ÜÂê´ÊúâFile IDÁöÑÊñáÊú¨Áõ¥Êé•ÂèëÈÄÅÁªôÊú∫Âô®‰∫∫  Âç≥ÂèØËøõË°åÊñá‰ª∂Ëß£Êûê",
        "ÂçïÊú∫Â§çÂà∂Ôºö", "Êñá‰ª∂Ëß£Á†ÅÂô®:", "ÊÇ®ÁöÑÊñá‰ª∂Á†ÅÂ∑≤ÁîüÊàêÔºåÁÇπÂáªÂ§çÂà∂Ôºö",
        "ÊâπÈáèÂèëÈÄÅÁöÑÂ™í‰Ωì‰ª£Á†ÅÂ¶Ç‰∏ã:", "Ê≠§Êù°Â™í‰ΩìÂàÜ‰∫´link:",
        "Â•≥‰æÖÊêúÁ¥¢Ôºö@ seefilebot", "Ëß£Á†ÅÔºö@ MediaBK2bot",
        "Â¶ÇÊûúÊÇ®Âè™ÊòØÊÉ≥Â§á‰ªΩÔºåÂèëÈÄÅ /settings ÂèØ‰ª•ËÆæÁΩÆÂÖ≥Èó≠Ê≠§Êù°ÂõûÂ§çÊ∂àÊÅØ",
        "Â™í‰ΩìÂåÖÂ∑≤ÂàõÂª∫ÔºÅ", "Ê≠§Â™í‰Ωì‰ª£Á†Å‰∏∫:", "Êñá‰ª∂ÂêçÁß∞:", "ÂàÜ‰∫´ÈìæÊé•:", "|_SendToBeach_|",
        "Forbidden: bot was kicked from the supergroup chat",
        "Bad Request: chat_id is empty"
    ]
    for text in replace_texts:
        original_string = original_string.replace(text, '')

    original_string = re.sub(r"ÂàÜ‰∫´Ëá≥\d{4}-\d{2}-\d{2} \d{2}:\d{2} Âà∞ÊúüÂêéÊÇ®‰ªçÂèØÈáçÊñ∞ÂàÜ‰∫´", '', original_string)

    json_pattern = r'\{[^{}]*?"text"\s*:\s*"[^"]+"[^{}]*?\}'
    matches = re.findall(json_pattern, original_string)
    for match in matches:
        try:
            data = json.loads(match)
            if 'content' in data and isinstance(data['content'], str):
                original_string += f"\n{data['content']}"
        except json.JSONDecodeError:
            pass
        original_string = original_string.replace(match, '')

    wp_patterns = [r'https://t\.me/[^\s]+']
    for pattern in wp_patterns:
        original_string = re.sub(pattern, '', original_string)

    for pat in [
        r'LINK\s*\n[^\n]+#C\d+\s*\nOriginal:[^\n]*\n?',
        r'LINK\s*\n[^\n]+#C\d+\s*\nForwarded from:[^\n]*\n?',
        r'LINK\s*\n[^\n]*#C\d+\s*',
        r'Original caption:[^\n]*\n?'
    ]:
        original_string = re.sub(pat, '', original_string)

    original_string = re.sub(r'^\s*$', '', original_string, flags=re.MULTILINE)
    lines = original_string.split('\n')
    unique_lines = list(dict.fromkeys(lines))
    result_string = "\n".join(lines)

    for symbol in ['üîë', 'üíé']:
        result_string = result_string.replace(symbol, '\r\n' + symbol)

    return result_string[:1500] if len(result_string) > 1500 else result_string

def replace_synonym(text):
    for k, v in SYNONYM.items():
        text = text.replace(k, v)
    return text

def segment_text(text):
    text = replace_synonym(text)
    return " ".join(jieba.cut(text))

def fetch_tag_cn_for_file(file_unique_id):
    return [
        t.tag_cn for t in Tag.select()
        .join(FileTag, on=(FileTag.tag == Tag.tag))
        .where(FileTag.file_unique_id == file_unique_id)
        if t.tag_cn
    ]

def sync_to_postgres(record):
    if not SYNC_TO_POSTGRES:
        return

    from playhouse.shortcuts import model_to_dict

    IGNORED_FIELDS = {'content_seg_tsv', 'created_at', 'updated_at'}

    model_data = model_to_dict(record, recurse=False)
    model_data = {k: v for k, v in model_data.items() if k not in IGNORED_FIELDS}
    model_data["id"] = record.id  # ÊòæÂºè‰∏ªÈîÆ

    with DB_PG.atomic():
        try:
            existing = SoraContentPg.get(SoraContentPg.id == record.id)
            for k, v in model_data.items():
                setattr(existing, k, v)
            existing.save()
        except SoraContentPg.DoesNotExist:
            SoraContentPg.create(**model_data)





def sync_media_to_postgres(content_id, media_rows):
    if not SYNC_TO_POSTGRES:
        return

    with DB_PG.atomic():
        for media in media_rows:
            insert_data = {
                "content_id": content_id,
                "source_bot_name": media["source_bot_name"],
                "file_id": media["file_id"],
                "thumb_file_id": media["thumb_file_id"]
            }
            print(f"Syncing media to PostgreSQL: {insert_data}")

            try:
                print(f"üõ∞Ô∏è Syncing media to PostgreSQL: {insert_data}")

                SoraMediaPg.insert(**insert_data).on_conflict(
                    conflict_target=[SoraMediaPg.content_id, SoraMediaPg.source_bot_name],
                    update={k: insert_data[k] for k in ['file_id', 'thumb_file_id']}
                ).execute()

            except Exception as e:
                print(f"‚ùå ÊèíÂÖ• PostgreSQL sora_media Â§±Ë¥•: {e}")
                print(f"   ‚û§ Â§±Ë¥•ÂÜÖÂÆπ: {insert_data}")




def process_documents():
    DB_MYSQL.connect()
    if SYNC_TO_POSTGRES:
        DB_PG.connect()

    for doc in Document.select().where((Document.kc_status.is_null(True)) | (Document.kc_status != 'updated')).limit(BATCH_LIMIT):
        if not doc.file_name and not doc.caption:
            doc.kc_status = 'updated'
            doc.save()
            continue

        content = clean_text(f"{doc.file_name or ''}\n{doc.caption or ''}")
        content_seg = segment_text(content)
        tag_cn_list = fetch_tag_cn_for_file(doc.file_unique_id)
        if tag_cn_list:
            content_seg += " " + " ".join(tag_cn_list)

        print(f"Processing {doc.file_unique_id}")

        if doc.kc_id:
            try:
                kw = SoraContent.get_by_id(doc.kc_id)
                kw.source_id = doc.file_unique_id
                kw.content = content
                kw.content_seg = content_seg
                kw.file_size = doc.file_size
                kw.save()
            except SoraContent.DoesNotExist:
                kw = SoraContent.create(
                    source_id=doc.file_unique_id, 
                    file_type='d', 
                    content=content, 
                    content_seg=content_seg,
                    file_size = doc.file_size
                    )
                doc.kc_id = kw.id
        else:
            kw = SoraContent.create(
                source_id=doc.file_unique_id, file_type='d', content=content, content_seg=content_seg,file_size = doc.file_size)
            doc.kc_id = kw.id

        doc.kc_status = 'updated'
        doc.save()

       
        if SYNC_TO_POSTGRES and kw.id:     
            sync_to_postgres(kw)

    DB_MYSQL.close()
    if SYNC_TO_POSTGRES:
        DB_PG.close()


def process_videos():
    DB_MYSQL.connect()
    if SYNC_TO_POSTGRES:
        DB_PG.connect()

    for doc in Video.select().where((Video.kc_status.is_null(True)) | (Video.kc_status != 'updated')).limit(BATCH_LIMIT):
        if not doc.file_name and not doc.caption:
            doc.kc_status = 'updated'
            doc.save()
            continue

        content = clean_text(f"{doc.file_name or ''}\n{doc.caption or ''}")
        content_seg = segment_text(content)
        tag_cn_list = fetch_tag_cn_for_file(doc.file_unique_id)
        if tag_cn_list:
            content_seg += " " + " ".join(tag_cn_list)

        print(f"Processing {doc.file_unique_id}: {content_seg}")

        if doc.kc_id:
            try:
                kw = SoraContent.get_by_id(doc.kc_id)
                kw.source_id = doc.file_unique_id
                kw.content = content
                kw.content_seg = content_seg
                kw.file_size = doc.file_size
                kw.duration = doc.duration
                kw.save()
            except SoraContent.DoesNotExist:
                kw = SoraContent.create(
                    source_id=doc.file_unique_id, 
                    file_type='v', 
                    content=content, 
                    content_seg=content_seg,
                    file_size = doc.file_size,
                    duration = doc.duration
                    )
                doc.kc_id = kw.id
        else:
            kw = SoraContent.create(
                source_id=doc.file_unique_id, 
                file_type='v', 
                content=content, 
                content_seg=content_seg,
                file_size = doc.file_size,
                duration = doc.duration
                )
            doc.kc_id = kw.id

        doc.kc_status = 'updated'
        doc.save()

       
        if SYNC_TO_POSTGRES and kw.id:     
            sync_to_postgres(kw)

    DB_MYSQL.close()
    if SYNC_TO_POSTGRES:
        DB_PG.close()




def process_sora_update():
    import time
    DB_MYSQL.connect()
    if SYNC_TO_POSTGRES:
        DB_PG.connect()

    sora_rows = Sora.select().where(Sora.update_content <= 0).limit(BATCH_LIMIT)
    print(f"üì¶ Ê≠£Âú®Â§ÑÁêÜ {len(sora_rows)} Á¨î sora Êï∞ÊçÆ...\n")

    for row in sora_rows:
        source_id = row.file_unique_id
        print(f"üîç Â§ÑÁêÜ source_id: {source_id}")

        content = {
            'source_id': source_id,
            'content': row.content or '',
            'owner_user_id': row.user_id,
            'source_channel_message_id': row.source_channel_message_id,
            'thumb_file_unique_id': row.thumb_file_unique_id,
            'thumb_hash': row.thumb_hash,
            'file_size': row.file_size,
            'duration': row.duration,
            'tag': row.tag,
            'file_type': row.file_type[0] if row.file_type else None,
            'plan_update_timestamp': row.plan_update_timestamp,
            'stage': row.stage
        }

        # ÊèíÂÖ•ÊàñÊõ¥Êñ∞ SoraContent
        sora_content, created = SoraContent.get_or_create(source_id=source_id, defaults=content)
        if created:
            print("‚úÖ Êñ∞Â¢û MySQL sora_content")
        else:
            for k, v in content.items():
                setattr(sora_content, k, v)
            sora_content.save()
            print("üîÑ Êõ¥Êñ∞ MySQL sora_content")

        # Âª∫Á´ã SoraMediaÔºà‰∏§‰∏™Êú∫Âô®‰∫∫Êù•Ê∫êÔºâ
        media_data = [
            {
                'source_bot_name': row.source_bot_name,
                'file_id': row.file_id,
                'thumb_file_id': row.thumb_file_id
            },
            {
                'source_bot_name': row.shell_bot_name,
                'file_id': row.shell_file_id,
                'thumb_file_id': row.shell_thumb_file_id
            }
        ]

        for media in media_data:
            existing = SoraMedia.select().where(
                (SoraMedia.content_id == sora_content.id) &
                (SoraMedia.source_bot_name == media["source_bot_name"])
            ).first()

            if existing:
                existing.file_id = media["file_id"]
                existing.thumb_file_id = media["thumb_file_id"]
                existing.save()
                print(f"  üîÑ Êõ¥Êñ∞ MySQL sora_media [{media['source_bot_name']}]")
            else:
                SoraMedia.create(content_id=sora_content.id, **media)
                print(f"  ‚úÖ Êñ∞Â¢û MySQL sora_media [{media['source_bot_name']}]")


        # Êõ¥Êñ∞ÂéüÂßãË°®Áä∂ÊÄÅ
        row.update_content = int(time.time())
        row.save()

        # ÂêåÊ≠•Âà∞ PostgreSQL
        if SYNC_TO_POSTGRES:
            sync_to_postgres(sora_content)
            sync_media_to_postgres(sora_content.id, media_data)
            print("üöÄ ÂêåÊ≠•Âà∞ PostgreSQL ÂÆåÊàê")

    DB_MYSQL.close()
    if SYNC_TO_POSTGRES:
        DB_PG.close()


if __name__ == "__main__":
    process_documents()
    process_videos()
    # process_sora_update()
